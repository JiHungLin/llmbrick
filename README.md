# LLMBrick

ä¸€å€‹æ¨¡çµ„åŒ–çš„ LLM æ‡‰ç”¨é–‹ç™¼æ¡†æ¶ï¼Œæ”¯æ´å¤šç¨®é€šä¿¡å”è­°å’Œå¯æ’æ‹”çš„çµ„ä»¶æ¶æ§‹ã€‚

## ç‰¹è‰²

- ğŸ§± **æ¨¡çµ„åŒ–è¨­è¨ˆ**: åŸºæ–¼ Brick çµ„ä»¶çš„å¯æ’æ‹”æ¶æ§‹
- ğŸ”„ **å¤šå”è­°æ”¯æ´**: SSEã€WebSocketã€WebRTCã€gRPC
- ğŸ¤– **å¤š LLM æ”¯æ´**: OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹
- ğŸ¤ **èªéŸ³è™•ç†**: ASR èªéŸ³è­˜åˆ¥æ•´åˆ
- ğŸ“š **RAG æ”¯æ´**: å…§å»ºæª¢ç´¢å¢å¼·ç”Ÿæˆ
- ğŸ”§ **æ˜“æ–¼æ“´å±•**: æ’ä»¶ç³»çµ±å’Œè‡ªå®šç¾©çµ„ä»¶

## å¿«é€Ÿé–‹å§‹

### å®‰è£

```bash
pip install llmbrick
```

### åŸºæœ¬ä½¿ç”¨

```python
from llmbrick import Pipeline, OpenAILLM
from llmbrick.servers.sse import SSEServer

# å»ºç«‹ Pipeline
pipeline = Pipeline()
pipeline.add_brick(OpenAILLM(api_key="your-api-key"))

# å•Ÿå‹• SSE æœå‹™
server = SSEServer(pipeline)
server.run(host="0.0.0.0", port=8000)
```

## ç¯„ä¾‹

#### é‹ä½œå–®å…ƒ Brick (ä½¿ç”¨å…§å»ºBrickï¼ŒDecoratorç›´æ¥æ›¿æ›è‡ªå·±çš„func)

```python
from llmbrick.core.brick import BaseBrick
import nest_asyncio

class LLMBrick(BaseBrick[str, str]):
    pass


llm = LLMBrick()

@llm.unary()
async def input(prompt: str) -> str:
    return f"user input: {prompt}"

result = await llm.run_unary("What is your name?") #ç›´æ¥èª¿ç”¨å°±æœ¬æ©Ÿé‹ç®—
```

#### é‹ä½œå–®å…ƒ Brick (ç¹¼æ‰¿Brickï¼Œå®¢è£½è‡ªå·±éœ€è¦çš„Brick)

```python
from llmbrick.core.brick import BaseBrick, unary_handler
import nest_asyncio


class MyNewBrick(BaseBrick[str, str]):
    def __init__(self, some_param: str, **kwargs):
        super().__init__(**kwargs)
        self.some_param = some_param
    
    @unary_handler
    async def process(self, input_data: str) -> str:
        return f"Processed: {input_data} with param {self.some_param}"
    
nest_asyncio.apply()

brick = MyNewBrick(some_param="example")

result = await brick.run_unary("What is your name? ") #ç›´æ¥èª¿ç”¨å°±æœ¬æ©Ÿé‹ç®—
```

#### Brickè½‰æ›ç‚ºç•°æ­¥ gRPC Server

```python
import asyncio
from llmbrick.servers.grpc.server import GrpcServer
from llmbrick.bricks.llm.base_llm import LLMBrick

async def main():
    # å»ºç«‹ LLM Brick
    brick = LLMBrick(default_prompt="ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹")
    
    # å»ºç«‹ç•°æ­¥ gRPC ä¼ºæœå™¨
    server = GrpcServer(port=50051)
    server.register_service(brick)
    
    # å•Ÿå‹•ç•°æ­¥ä¼ºæœå™¨
    await server.start()

# é‹è¡Œä¼ºæœå™¨
asyncio.run(main())
```

#### Brickè½‰æ›ç‚ºç•°æ­¥ gRPC Client

```python
import asyncio
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.protocols.models.bricks.llm_types import LLMRequest

async def main():
    # å»ºç«‹ç•°æ­¥ gRPC å®¢æˆ¶ç«¯
    brick = LLMBrick.toGrpcClient(remote_address="127.0.0.1:50051")
    
    # å–®æ¬¡è«‹æ±‚ - è·Ÿæœ¬æ©Ÿèª¿ç”¨çš„å¯«æ³•ä¸€æ¨£
    request = LLMRequest(prompt="What is your name?")
    result = await brick.run_unary(request)
    print(result)
    
    # æµå¼è«‹æ±‚
    async for chunk in brick.run_output_streaming(request):
        print(chunk)
    
    # æ¸…ç†è³‡æº
    await brick._grpc_channel.close()

# é‹è¡Œå®¢æˆ¶ç«¯
asyncio.run(main())
```

#### å»ºç«‹SSEæ¥å£

```python
from llmbrick.servers.sse.server import SSEServer
import asyncio

server = SSEServer() 
# æœƒè‡ªå‹•å»ºç«‹SSEçš„Router

fast_app = server.fastapi_app # é€™ç­‰åŒFastAPIçš„app
# ç­‰åƒ¹ app = FastAPI()

@server.handler # ä½¿ç”¨Decoratorè‡ªè¨‚éœ€è¦çš„é‚è¼¯ï¼Œé€™é‚Šå°±æ˜¯æ•´åˆæ‰€æœ‰LLM Brickçš„å€å¡Š
async def simple_flow(request_body):
    # æ¨¡æ“¬è¨Šæ¯è™•ç†èˆ‡å›æ‡‰
    yield {"id": "1", "type": "text", "text": "Hello, this is a streaming response.", "progress": "IN_PROGRESS"}
    await asyncio.sleep(0.5)
    yield {"id": "1", "type": "done", "progress": "DONE"}

if __name__ == "__main__":
    server.run(host="0.0.0.0", port=8000)
```

#### å»ºç«‹SSEæ¥å£ï¼Œæ­é…Brick

```python
from llmbrick.servers.sse.server import SSEServer
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.bricks.intention.base_intention import IntentionBrick
import asyncio

server = SSEServer() 
# æœƒè‡ªå‹•å»ºç«‹SSEçš„Router

fast_app = server.fastapi_app # é€™ç­‰åŒFastAPIçš„app
# ç­‰åƒ¹ app = FastAPI()

intention_brick = IntentionBrick()
llm_brick = LLMBrick.toGrpcClient(remote_address="192.168.1.100:50051")

@server.handler #ä½¿ç”¨Decoratorè‡ªè¨‚éœ€è¦çš„é‚è¼¯ï¼Œé€™é‚Šå°±æ˜¯æ•´åˆæ‰€æœ‰LLM Brickçš„å€å¡Š
async def simple_flow(request_body):
    # æ¨¡æ“¬è¨Šæ¯è™•ç†èˆ‡å›æ‡‰
    text = request_body.text;
    
    intention_list = await intention_brick.run_unary(text)
    
    ... # è‡ªè¨‚ç´°ç¯€

    input_data = {
        "intention_list": intention_list,
        "model_id": request_body.modelId,
        "max_tokens": request_body.maxTokens,
        "context": [...]
    }
    try:
        for i in llm_brick.run_output_stream(input_data):
            output = {"event": "message", "data": i.text}
            yield output
    except:
        yield {"event": "done"}
    
    yield {"event": "done"}
        

if __name__ == "__main__":
    server.run(host="0.0.0.0", port=8000)
```

## æ–‡æª”

- [å¿«é€Ÿé–‹å§‹](docs/quickstart.md)
- [API åƒè€ƒ](docs/api_reference/)
- [æ•™å­¸ç¯„ä¾‹](docs/tutorials/)

## æˆæ¬Š

MIT License