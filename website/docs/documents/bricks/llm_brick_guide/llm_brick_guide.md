# LLMBrick å®Œæ•´ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è©³ç´°èªªæ˜ [`llmbrick/bricks/llm/base_llm.py`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L14) ä¸­çš„ LLMBrick å¯¦ä½œï¼Œé€™æ˜¯ LLMBrick æ¡†æ¶ä¸­å°ˆç‚ºå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‡‰ç”¨è¨­è¨ˆçš„æ ¸å¿ƒçµ„ä»¶ã€‚

---

## å°ˆæ¡ˆæ¦‚è¿°èˆ‡ç›®æ¨™

### ğŸ¯ è¨­è¨ˆç›®æ¨™èˆ‡è§£æ±ºå•é¡Œ

LLMBrick æ—¨åœ¨è§£æ±ºä»¥ä¸‹ LLM æ‡‰ç”¨é–‹ç™¼çš„ç—›é»ï¼š

- **æ¨™æº–åŒ– LLM è«‹æ±‚/å›æ‡‰æµç¨‹**ï¼šçµ±ä¸€ promptã€contextã€æµå¼å›æ‡‰ç­‰å¸¸è¦‹ LLM äº’å‹•æ¨¡å¼
- **gRPC æœå‹™åŒ–**ï¼šå…§å»º gRPC å”å®šï¼Œæ”¯æ´å–®æ¬¡èˆ‡æµå¼å›æ‡‰
- **æ˜“æ–¼æ“´å±•èˆ‡å®¢è£½åŒ–**ï¼šå¯è‡ªè¨‚ prompt è™•ç†ã€æ¨¡å‹é¸æ“‡ã€å›æ‡‰æ ¼å¼
- **èˆ‡ CommonBrick å®Œå…¨ç›¸å®¹**ï¼šç¹¼æ‰¿æ‰€æœ‰é€šç”¨éŒ¯èª¤è™•ç†ã€æœå‹™è³‡è¨ŠæŸ¥è©¢ç­‰èƒ½åŠ›

---

## å°ˆæ¡ˆçµæ§‹åœ–èˆ‡æ¨¡çµ„è©³è§£

### æ¶æ§‹åœ–

```plaintext
LLMBrick Framework
â”œâ”€â”€ llmbrick/
â”‚   â”œâ”€â”€ bricks/
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ base_llm.py         # LLMBrick ä¸»é«”å¯¦ä½œ
â”‚   â”œâ”€â”€ protocols/
â”‚   â”‚   â”œâ”€â”€ grpc/
â”‚   â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚   â”‚       â”œâ”€â”€ llm.proto       # Protocol Buffer å®šç¾©
â”‚   â”‚   â”‚       â”œâ”€â”€ llm_pb2.py      # è‡ªå‹•ç”Ÿæˆçš„è¨Šæ¯é¡åˆ¥
â”‚   â”‚   â”‚       â””â”€â”€ llm_pb2_grpc.py # gRPC æœå‹™å­˜æ ¹
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ bricks/
â”‚   â”‚           â””â”€â”€ llm_types.py    # LLMRequest/LLMResponse è³‡æ–™æ¨¡å‹
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ brick.py                # BaseBrick æŠ½è±¡åŸºç¤é¡åˆ¥
```

### æ ¸å¿ƒæ¨¡çµ„èªªæ˜

#### 1. [`LLMBrick`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L14) - LLM å°ˆç”¨ Brick

- **è·è²¬**ï¼šå°ˆç‚º LLM æ‡‰ç”¨è¨­è¨ˆï¼Œæ¨™æº–åŒ– promptã€contextã€æµå¼å›æ‡‰ç­‰äº’å‹•æ¨¡å¼
- **ç¹¼æ‰¿è‡ª**ï¼š[`BaseBrick`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/core/brick.py)
- **gRPC æœå‹™é¡å‹**ï¼š`llm`
- **åƒ…å…è¨±ä¸‰ç¨® handler**ï¼šunaryã€output_streamingã€get_service_info

#### 2. gRPC å”å®šå±¤

- **[llm.proto](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/grpc/llm/llm.proto#L33)** å®šç¾©
    ```protobuf
    service LLMService {
      rpc GetServiceInfo(ServiceInfoRequest) returns (ServiceInfoResponse);
      rpc Unary(LLMRequest) returns (LLMResponse);
      rpc OutputStreaming(LLMRequest) returns (stream LLMResponse);
    }
    ```
- **è¨Šæ¯çµæ§‹**ï¼š
    - `LLMRequest`ï¼šmodel_id, prompt, context, client_id, session_id, request_id, source_language, temperature, max_tokens
    - `LLMResponse`ï¼štext, tokens, is_final, error

#### 3. è³‡æ–™æ¨¡å‹

- **[`LLMRequest`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L24)**ï¼šå°è£ LLM è«‹æ±‚åƒæ•¸
- **[`LLMResponse`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L72)**ï¼šå°è£ LLM å›æ‡‰å…§å®¹
- **[`Context`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L11)**ï¼šå°è©±ä¸Šä¸‹æ–‡

---

## å®‰è£èˆ‡ç’°å¢ƒè¨­å®šæŒ‡å—

### ä¾è³´éœ€æ±‚

LLMBrick éœ€è¦ä»¥ä¸‹æ ¸å¿ƒä¾è³´ï¼š

```bash
pip install llmbrick
# æœƒè‡ªå‹•å®‰è£ grpcioã€protobuf ç­‰å¿…è¦å¥—ä»¶
```

### è‡ªå‹•åŒ–å®‰è£æ­¥é©Ÿ

1. **å®‰è£ LLMBrick å¥—ä»¶**
    ```bash
    pip install llmbrick
    ```
2. **é©—è­‰å®‰è£**
    ```python
    from llmbrick.bricks.llm.base_llm import LLMBrick
    print("âœ… LLMBrick å®‰è£æˆåŠŸï¼")
    ```
3. **é–‹ç™¼ç’°å¢ƒå»ºè­°**
    ```bash
    pip install -r requirements-dev.txt
    export LLMBRICK_LOG_LEVEL=INFO
    export LLMBRICK_GRPC_PORT=50051
    ```

---

## é€æ­¥ç¯„ä¾‹ï¼šå¾åŸºç¤åˆ°é€²éš

### 1. æœ€ç°¡å–®çš„ LLMBrick ä½¿ç”¨

```python
import asyncio
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.core.brick import unary_handler, get_service_info_handler
from llmbrick.protocols.models.bricks.llm_types import LLMRequest, LLMResponse, Context
from llmbrick.protocols.models.bricks.common_types import ErrorDetail, ServiceInfoResponse

class SimpleLLMBrick(LLMBrick):
    def __init__(self, default_prompt="Say hi", **kwargs):
        super().__init__(default_prompt=default_prompt, **kwargs)

    @unary_handler
    async def echo(self, request: LLMRequest) -> LLMResponse:
        return LLMResponse(
            text=f"Echo: {request.prompt or self.default_prompt}",
            tokens=["echo"],
            is_final=True,
            error=ErrorDetail(code=200, message="Success"),
        )

    @get_service_info_handler
    async def info(self) -> ServiceInfoResponse:
        return ServiceInfoResponse(
            service_name="SimpleLLMBrick",
            version="1.0.0",
            models=[],
            error=ErrorDetail(code=200, message="Success"),
        )

async def main():
    brick = SimpleLLMBrick(default_prompt="Hello")
    req = LLMRequest(prompt="Test prompt", context=[])
    resp = await brick.run_unary(req)
    print(resp.text)

asyncio.run(main())
```

### 2. æµå¼å›æ‡‰èˆ‡æœå‹™è³‡è¨Š

```python
from llmbrick.core.brick import output_streaming_handler

class StreamLLMBrick(LLMBrick):
    def __init__(self, default_prompt="Stream!", **kwargs):
        super().__init__(default_prompt=default_prompt, **kwargs)

    @output_streaming_handler
    async def stream(self, request: LLMRequest):
        for i, word in enumerate((request.prompt or self.default_prompt).split()):
            yield LLMResponse(
                text=word,
                tokens=[word],
                is_final=(i == len((request.prompt or self.default_prompt).split()) - 1),
                error=None,
            )
```

### 3. gRPC å®¢æˆ¶ç«¯é€£æ¥èˆ‡ä½¿ç”¨

```python
import asyncio
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.protocols.models.bricks.llm_types import LLMRequest

async def grpc_client_example():
    client = LLMBrick.toGrpcClient("localhost:50051", default_prompt="Hi!")
    req = LLMRequest(prompt="gRPC test", context=[])
    resp = await client.run_unary(req)
    print(resp.text)

asyncio.run(grpc_client_example())
```

æ›´å¤šç¯„ä¾‹è«‹åƒè€ƒ [examples/llm_brick_define](https://github.com/JiHungLin/llmbrick/tree/main/examples/llm_brick_define)ã€‚

---

## æ ¸å¿ƒ API / é¡åˆ¥ / å‡½å¼æ·±åº¦è§£æ

### [`LLMBrick`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L14) é¡åˆ¥

#### é¡åˆ¥ç°½åèˆ‡ç¹¼æ‰¿é—œä¿‚

```python
class LLMBrick(BaseBrick[LLMRequest, LLMResponse]):
    brick_type = BrickType.LLM
    allowed_handler_types = {"unary", "output_streaming", "get_service_info"}
```

#### é‡è¦å±¬æ€§

- `default_prompt: str` - é è¨­æç¤ºè©
- `brick_type` - æ¨™è­˜ç‚º LLM é¡å‹
- `allowed_handler_types` - åƒ…å…è¨±ä¸‰ç¨® handler

#### ä¸»è¦æ–¹æ³•

##### [`toGrpcClient()`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L61)

- **åŠŸèƒ½**ï¼šå°‡ LLMBrick è½‰æ›ç‚ºç•°æ­¥ gRPC å®¢æˆ¶ç«¯
- **åƒæ•¸**ï¼š
    - `remote_address: str` - gRPC ä¼ºæœå™¨ä½å€
    - `default_prompt: str` - é è¨­æç¤ºè©
    - `**kwargs` - é¡å¤–åˆå§‹åŒ–åƒæ•¸
- **å›å‚³**ï¼šé…ç½®ç‚º gRPC å®¢æˆ¶ç«¯çš„ LLMBrick å¯¦ä¾‹
- **ç¯„ä¾‹**ï¼š
    ```python
    client = LLMBrick.toGrpcClient("localhost:50051", default_prompt="Hi!")
    ```

##### [`run_unary()`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/core/brick.py#L233)

- **åŠŸèƒ½**ï¼šåŸ·è¡Œå–®æ¬¡ LLM è«‹æ±‚
- **åƒæ•¸**ï¼š`input_data: LLMRequest`
- **å›å‚³**ï¼š`LLMResponse`
- **ç¯„ä¾‹**ï¼š
    ```python
    req = LLMRequest(prompt="Hello", context=[])
    resp = await brick.run_unary(req)
    ```

##### [`run_output_streaming()`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/core/brick.py#L258)

- **åŠŸèƒ½**ï¼šåŸ·è¡Œæµå¼ LLM è¼¸å‡º
- **åƒæ•¸**ï¼š`input_data: LLMRequest`
- **å›å‚³**ï¼š`AsyncIterator[LLMResponse]`
- **ç¯„ä¾‹**ï¼š
    ```python
    async for resp in brick.run_output_streaming(req):
        print(resp.text)
    ```

##### [`run_get_service_info()`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/core/brick.py#L245)

- **åŠŸèƒ½**ï¼šæŸ¥è©¢æœå‹™è³‡è¨Š
- **å›å‚³**ï¼š`ServiceInfoResponse`

#### ä¸æ”¯æ´çš„ handler

- LLMBrick **ä¸æ”¯æ´** input_streaming èˆ‡ bidi_streamingï¼Œèª¿ç”¨æœƒæ‹‹å‡º NotImplementedError
    - [`bidi_streaming`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L39)
    - [`input_streaming`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py#L51)

---

## è³‡æ–™æ¨¡å‹èªªæ˜

### [`LLMRequest`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L24)

- `model_id: str` - æŒ‡å®šæ¨¡å‹
- `prompt: str` - è¼¸å…¥æç¤ºè©
- `context: List[Context]` - å°è©±ä¸Šä¸‹æ–‡
- `client_id/session_id/request_id/source_language/temperature/max_tokens` - é€²éšåƒæ•¸

### [`LLMResponse`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L72)

- `text: str` - å›æ‡‰æ–‡å­—
- `tokens: List[str]` - åˆ†è©çµæœï¼ˆæµå¼æ™‚å¯ç”¨ï¼‰
- `is_final: bool` - æ˜¯å¦ç‚ºæœ€å¾Œä¸€ç­†
- `error: Optional[ErrorDetail]` - éŒ¯èª¤è³‡è¨Š

### [`Context`](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py#L11)

- `role: str` - è§’è‰²ï¼ˆå¦‚ user/assistantï¼‰
- `content: str` - å…§å®¹

---

## å¸¸è¦‹éŒ¯èª¤èˆ‡æ’é™¤

- **TypeError: context å¿…é ˆç‚º List[Context]**
    - è«‹ç¢ºä¿ LLMRequest çš„ context æ¬„ä½ç‚º Context ç‰©ä»¶åˆ—è¡¨
- **NotImplementedError: LLMBrick does not support input_streaming/bidi_streaming handler**
    - LLMBrick åƒ…æ”¯æ´ unaryã€output_streamingã€get_service_info
- **gRPC é€£ç·šå¤±æ•—**
    - æª¢æŸ¥ä¼ºæœå™¨ä½å€èˆ‡é˜²ç«ç‰†è¨­å®š
- **tokens æ¬„ä½å‹åˆ¥éŒ¯èª¤**
    - tokens å¿…é ˆç‚º List[str]

---

## æ•ˆèƒ½å„ªåŒ–èˆ‡æœ€ä½³å¯¦è¸

- **åƒ…è¨»å†Šå…è¨±çš„ handler**ï¼šLLMBrick åªå…è¨± unaryã€output_streamingã€get_service_info
- **å–„ç”¨æµå¼å›æ‡‰**ï¼šé•·æ–‡æœ¬å»ºè­°ç”¨ output_streaming æå‡ç”¨æˆ¶é«”é©—
- **æœå‹™è³‡è¨Šè‡ªå‹•åŒ–**ï¼šå»ºè­°å¯¦ä½œ get_service_info_handlerï¼Œæ–¹ä¾¿å‰ç«¯è‡ªå‹•ç™¼ç¾æ¨¡å‹èƒ½åŠ›
- **å‹åˆ¥å®‰å…¨**ï¼šæ‰€æœ‰è³‡æ–™çµæ§‹è«‹ç”¨ LLMRequest/LLMResponse/Context

---

## FAQ / é€²éšå•ç­”

### Q1: LLMBrick èˆ‡ CommonBrick å·®ç•°ï¼Ÿ

**A**ï¼šLLMBrick å°ˆç‚º LLM æ‡‰ç”¨è¨­è¨ˆï¼Œåƒ…å…è¨± prompt/context ç›¸é—œçš„ä¸‰ç¨® handlerï¼Œä¸”è³‡æ–™æ¨¡å‹æ›´åš´è¬¹ã€‚CommonBrick å‰‡ç‚ºé€šç”¨å‹ï¼Œå…è¨±æ‰€æœ‰é€šè¨Šæ¨¡å¼ã€‚

### Q2: å¦‚ä½•ä¸²æ¥å¤–éƒ¨ LLMï¼ˆå¦‚ OpenAIï¼‰ï¼Ÿ

**A**ï¼šå¯ç¹¼æ‰¿ LLMBrickï¼Œæ–¼ unary/output_streaming handler å…§å‘¼å«å¤–éƒ¨ APIï¼Œä¸¦å°‡å›æ‡‰åŒ…è£ç‚º LLMResponseã€‚

### Q3: å¯ä»¥è‡ªè¨‚ context è™•ç†å—ï¼Ÿ

**A**ï¼šå¯ä»¥ï¼Œcontext æ¬„ä½ç‚º List[Context]ï¼Œå¯ä¾éœ€æ±‚è‡ªè¨‚å°è©±æ­·å²æ ¼å¼èˆ‡è™•ç†é‚è¼¯ã€‚

---

## åƒè€ƒè³‡æºèˆ‡å»¶ä¼¸é–±è®€

- [LLMBrick GitHub åŸå§‹ç¢¼](https://github.com/JiHungLin/llmbrick)
- [llmbrick/bricks/llm/base_llm.py](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/bricks/llm/base_llm.py)
- [llmbrick/protocols/grpc/llm/llm.proto](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/grpc/llm/llm.proto)
- [llmbrick/protocols/models/bricks/llm_types.py](https://github.com/JiHungLin/llmbrick/blob/main/llmbrick/protocols/models/bricks/llm_types.py)
- [å®˜æ–¹ç¯„ä¾‹ç¨‹å¼ç¢¼](https://github.com/JiHungLin/llmbrick/tree/main/examples/llm_brick_define)
- [gRPC Python å®˜æ–¹æ–‡ä»¶](https://grpc.io/docs/languages/python/)
- [Protocol Buffer å®˜æ–¹æ–‡ä»¶](https://developers.google.com/protocol-buffers)

---

LLMBrick æ˜¯æ§‹å»ºç¾ä»£ LLM æ‡‰ç”¨çš„æœ€ä½³èµ·é»ï¼ŒæŒæ¡å…¶ç”¨æ³•èƒ½å¤§å¹…æå‡é–‹ç™¼æ•ˆç‡èˆ‡ç¶­è­·æ€§ã€‚å¦‚æœ‰å•é¡Œï¼Œæ­¡è¿åƒèˆ‡ç¤¾ç¾¤è¨è«–æˆ–å›å ± issueï¼
