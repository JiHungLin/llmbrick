"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[669],{3812:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>l,default:()=>a,frontMatter:()=>o,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"quickstart/llm_brick_define","title":"llm_brick_define","description":"GitHub \u7bc4\u4f8b\u7a0b\u5f0f\u78bc","source":"@site/docs/quickstart/llm_brick_define.md","sourceDirName":"quickstart","slug":"/quickstart/llm_brick_define","permalink":"/llmbrick/docs/quickstart/llm_brick_define","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"sidebar_label":"LLMBrick \u5b9a\u7fa9"},"sidebar":"quickstartSidebar","previous":{"title":"\u610f\u5716\u5206\u985e IntentionBrick \u5b9a\u7fa9","permalink":"/llmbrick/docs/quickstart/intention_brick_define"},"next":{"title":"\u6aa2\u7d22\u578b RetrievalBrick \u5b9a\u7fa9","permalink":"/llmbrick/docs/quickstart/retrieval_brick_define"}}');var i=n(4848),s=n(8453);const o={sidebar_position:5,sidebar_label:"LLMBrick \u5b9a\u7fa9"},l="\u5b9a\u7fa9\u8207\u4f7f\u7528 LLMBrick",c={},p=[{value:"1. \u4ec0\u9ebc\u662f LLMBrick\uff1f",id:"1-\u4ec0\u9ebc\u662f-llmbrick",level:2},{value:"2. \u5be6\u4f5c\u81ea\u8a02 LLMBrick",id:"2-\u5be6\u4f5c\u81ea\u8a02-llmbrick",level:2},{value:"3. \u672c\u5730\u7aef\u547c\u53eb\u7bc4\u4f8b",id:"3-\u672c\u5730\u7aef\u547c\u53eb\u7bc4\u4f8b",level:2},{value:"\u5e38\u898b\u932f\u8aa4\u8655\u7406",id:"\u5e38\u898b\u932f\u8aa4\u8655\u7406",level:3},{value:"4. \u4ee5 gRPC \u65b9\u5f0f\u63d0\u4f9b\u670d\u52d9",id:"4-\u4ee5-grpc-\u65b9\u5f0f\u63d0\u4f9b\u670d\u52d9",level:2},{value:"\u555f\u52d5 gRPC \u4f3a\u670d\u5668",id:"\u555f\u52d5-grpc-\u4f3a\u670d\u5668",level:3},{value:"5. \u4ee5 gRPC Client \u547c\u53eb\u9060\u7aef LLMBrick",id:"5-\u4ee5-grpc-client-\u547c\u53eb\u9060\u7aef-llmbrick",level:2},{value:"6. \u65b9\u6cd5\u578b\u614b\u7e3d\u89bd",id:"6-\u65b9\u6cd5\u578b\u614b\u7e3d\u89bd",level:2},{value:"7. \u5be6\u4f5c\u5efa\u8b70\u8207\u6700\u4f73\u5be6\u8e10",id:"7-\u5be6\u4f5c\u5efa\u8b70\u8207\u6700\u4f73\u5be6\u8e10",level:2},{value:"8. \u5b8c\u6574\u7bc4\u4f8b\u7a0b\u5f0f\u78bc",id:"8-\u5b8c\u6574\u7bc4\u4f8b\u7a0b\u5f0f\u78bc",level:2},{value:"9. \u5e38\u898b\u554f\u984c",id:"9-\u5e38\u898b\u554f\u984c",level:2}];function d(e){const r={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"https://github.com/JiHungLin/llmbrick/tree/main/examples/llm_brick_define",children:"GitHub \u7bc4\u4f8b\u7a0b\u5f0f\u78bc"})}),"\n",(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"\u5b9a\u7fa9\u8207\u4f7f\u7528-llmbrick",children:"\u5b9a\u7fa9\u8207\u4f7f\u7528 LLMBrick"})}),"\n",(0,i.jsx)(r.p,{children:"\u672c\u6559\u5b78\u5c07\u8aaa\u660e\u5982\u4f55\u5728 LLMBrick \u6846\u67b6\u4e2d\u81ea\u8a02\u3001\u5be6\u4f5c\u4e26\u4f7f\u7528 LLMBrick\uff0c\u6db5\u84cb\u672c\u5730\u7aef\u547c\u53eb\u8207 gRPC \u670d\u52d9\u5169\u7a2e\u60c5\u5883\uff0c\u4e26\u91dd\u5c0d\u5e38\u898b\u65b9\u6cd5\u578b\u614b\uff08Unary\u3001Output Streaming\u3001Service Info\uff09\u63d0\u4f9b\u5b8c\u6574\u7bc4\u4f8b\u8207\u8aaa\u660e\u3002"}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"1-\u4ec0\u9ebc\u662f-llmbrick",children:"1. \u4ec0\u9ebc\u662f LLMBrick\uff1f"}),"\n",(0,i.jsxs)(r.p,{children:["LLMBrick \u662f\u5c08\u70ba\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u61c9\u7528\u8a2d\u8a08\u7684 Brick \u985e\u578b\uff0c\u9069\u5408\u7528\u4f86\u5be6\u4f5c\u5404\u7a2e\u81ea\u8a02\u7684 LLM \u529f\u80fd\u6a21\u7d44\u3002",(0,i.jsx)(r.br,{}),"\n","LLMBrick \u9810\u8a2d\u652f\u63f4 prompt\u3001context\u3001\u6d41\u5f0f\u56de\u61c9\u7b49\u5e38\u898b LLM \u4e92\u52d5\u6a21\u5f0f\uff0c\u4e26\u53ef\u8f15\u9b06\u4e32\u63a5\u81f3\u672c\u5730\u6216\u9060\u7aef\u670d\u52d9\u3002"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"2-\u5be6\u4f5c\u81ea\u8a02-llmbrick",children:"2. \u5be6\u4f5c\u81ea\u8a02 LLMBrick"}),"\n",(0,i.jsxs)(r.p,{children:["\u9996\u5148\uff0c\u5efa\u7acb\u4e00\u500b\u7e7c\u627f\u81ea ",(0,i.jsx)(r.code,{children:"LLMBrick"})," \u7684\u81ea\u8a02\u985e\u5225\uff0c\u4e26\u5be6\u4f5c\u5404\u7a2e\u65b9\u6cd5\uff1a"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# examples/llm_brick_define/my_brick.py\nfrom typing import List, AsyncIterator\nfrom llmbrick.bricks.llm.base_llm import LLMBrick\nfrom llmbrick.protocols.models.bricks.llm_types import LLMRequest, LLMResponse, Context\nfrom llmbrick.protocols.models.bricks.common_types import ErrorDetail, ServiceInfoResponse, ModelInfo\nfrom llmbrick.core.error_codes import ErrorCodes\nfrom llmbrick.core.brick import unary_handler, output_streaming_handler, get_service_info_handler\n\nclass MyLLMBrick(LLMBrick):\n    """\n    MyLLMBrick \u662f LLMBrick \u7684\u81ea\u8a02\u7bc4\u4f8b\uff0c\u5c55\u793a echo\u3001\u6d41\u5f0f\u56de\u61c9\u8207\u670d\u52d9\u8cc7\u8a0a\u67e5\u8a62\u3002\n    \u652f\u63f4 default_prompt\u3001model_id\u3001supported_languages\u3001version\u3001description \u7b49\u521d\u59cb\u5316\u53c3\u6578\u3002\n    """\n    def __init__(\n        self,\n        default_prompt: str = "Say something",\n        model_id: str = "my-llm-model",\n        supported_languages: List[str] = None,\n        version: str = "1.0.0",\n        description: str = "A simple LLMBrick example that echoes prompt and streams output.",\n        **kwargs\n    ):\n        super().__init__(default_prompt=default_prompt, **kwargs)\n        self.model_id = model_id\n        self.supported_languages = supported_languages or ["en", "zh"]\n        self.version = version\n        self.description = description\n\n    @unary_handler\n    async def echo(self, request: LLMRequest) -> LLMResponse:\n        """\n        \u55ae\u6b21\u8acb\u6c42-\u56de\u61c9\uff1a\u56de\u50b3 prompt \u6216 default_prompt\uff0ctokens \u70ba\u5b57\u4e32\u5217\u8868\u3002\n        """\n        prompt = request.prompt or self.default_prompt\n        if not isinstance(request.context, list):\n            error = ErrorDetail(\n                code=ErrorCodes.PARAMETER_INVALID,\n                message="context \u5fc5\u9808\u70ba List[Context]"\n            )\n            return LLMResponse(text="", tokens=[], is_final=True, error=error)\n        if not prompt:\n            error = ErrorDetail(\n                code=ErrorCodes.PARAMETER_INVALID,\n                message="prompt \u4e0d\u53ef\u70ba\u7a7a"\n            )\n            return LLMResponse(text="", tokens=[], is_final=True, error=error)\n        tokens = prompt.split()\n        return LLMResponse(\n            text=f"Echo: {prompt}",\n            tokens=tokens,\n            is_final=True,\n            error=ErrorDetail(code=ErrorCodes.SUCCESS, message="Success")\n        )\n\n    @output_streaming_handler\n    async def stream(self, request: LLMRequest) -> AsyncIterator[LLMResponse]:\n        """\n        \u55ae\u6b21\u8acb\u6c42-\u6d41\u5f0f\u56de\u61c9\uff1a\u5c07 prompt \u62c6\u6210\u591a\u6bb5\u6d41\u5f0f\u56de\u50b3\u3002\n        """\n        prompt = request.prompt or self.default_prompt\n        if not prompt:\n            yield LLMResponse(\n                text="",\n                tokens=[],\n                is_final=True,\n                error=ErrorDetail(code=ErrorCodes.PARAMETER_INVALID, message="prompt \u4e0d\u53ef\u70ba\u7a7a")\n            )\n            return\n        words = prompt.split()\n        for idx, word in enumerate(words):\n            yield LLMResponse(\n                text=word,\n                tokens=[word],\n                is_final=(idx == len(words) - 1),\n                error=ErrorDetail(code=ErrorCodes.SUCCESS, message="Success")\n            )\n\n    @get_service_info_handler\n    async def info(self) -> ServiceInfoResponse:\n        """\n        \u56de\u50b3\u672c Brick \u7684\u670d\u52d9\u8cc7\u8a0a\u3002\n        """\n        model_info = ModelInfo(\n            model_id=self.model_id,\n            version=self.version,\n            supported_languages=self.supported_languages,\n            support_streaming=True,\n            description=self.description\n        )\n        return ServiceInfoResponse(\n            service_name="MyLLMBrick",\n            version=self.version,\n            models=[model_info],\n            error=ErrorDetail(code=ErrorCodes.SUCCESS, message="Success")\n        )\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"3-\u672c\u5730\u7aef\u547c\u53eb\u7bc4\u4f8b",children:"3. \u672c\u5730\u7aef\u547c\u53eb\u7bc4\u4f8b"}),"\n",(0,i.jsx)(r.p,{children:"\u76f4\u63a5\u65bc Python \u7a0b\u5f0f\u4e2d\u5be6\u4f8b\u5316\u4e26\u547c\u53eb LLMBrick\uff0c\u9069\u5408\u55ae\u5143\u6e2c\u8a66\u6216\u5d4c\u5165\u5f0f\u61c9\u7528\uff1a"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# examples/llm_brick_define/local_use.py\nimport asyncio\nfrom my_brick import MyLLMBrick\nfrom llmbrick.protocols.models.bricks.llm_types import LLMRequest, Context\n\nasync def main():\n    brick = MyLLMBrick(default_prompt="Hello LLM", model_id="local-llm", supported_languages=["en", "zh"])\n\n    print("=== Get Service Info ===")\n    try:\n        info = await brick.run_get_service_info()\n        print(info)\n    except Exception as e:\n        print(f"Error in get_service_info: {e}")\n\n    print("\\n=== Unary Method ===")\n    try:\n        print("Normal case:")\n        req = LLMRequest(prompt="Test prompt", context=[Context(role="user", content="Hi")])\n        resp = await brick.run_unary(req)\n        print(resp)\n\n        print("Error case (empty prompt):")\n        req = LLMRequest(prompt="", context=[Context(role="user", content="Hi")])\n        resp = await brick.run_unary(req)\n        print(resp)\n\n        print("Error case (context type error):")\n        req = LLMRequest(prompt="Test", context=None)  # type: ignore\n        resp = await brick.run_unary(req)\n        print(resp)\n    except Exception as e:\n        print(f"Error in unary call: {e}")\n\n    print("\\n=== Output Streaming Method ===")\n    try:\n        print("Normal case:")\n        req = LLMRequest(prompt="Stream this text", context=[])\n        async for resp in brick.run_output_streaming(req):\n            await asyncio.sleep(0.2)\n            print(resp)\n\n        print("Error case (empty prompt):")\n        req = LLMRequest(prompt="", context=[])\n        async for resp in brick.run_output_streaming(req):\n            await asyncio.sleep(0.2)\n            print(resp)\n    except Exception as e:\n        print(f"Error in output streaming: {e}")\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,i.jsx)(r.h3,{id:"\u5e38\u898b\u932f\u8aa4\u8655\u7406",children:"\u5e38\u898b\u932f\u8aa4\u8655\u7406"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["\u8f38\u5165\u8cc7\u6599\u4e0d\u6b63\u78ba\u6642\uff0c\u6703\u56de\u50b3\u5e36\u6709 ",(0,i.jsx)(r.code,{children:"error"})," \u6b04\u4f4d\u7684 ",(0,i.jsx)(r.code,{children:"LLMResponse"}),"\uff0c\u53ef\u64da\u6b64\u9032\u884c\u4f8b\u5916\u8655\u7406\u3002"]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"4-\u4ee5-grpc-\u65b9\u5f0f\u63d0\u4f9b\u670d\u52d9",children:"4. \u4ee5 gRPC \u65b9\u5f0f\u63d0\u4f9b\u670d\u52d9"}),"\n",(0,i.jsx)(r.h3,{id:"\u555f\u52d5-grpc-\u4f3a\u670d\u5668",children:"\u555f\u52d5 gRPC \u4f3a\u670d\u5668"}),"\n",(0,i.jsx)(r.p,{children:"\u5c07\u81ea\u8a02 LLMBrick \u8a3b\u518a\u5230 gRPC \u4f3a\u670d\u5668\uff0c\u5c0d\u5916\u63d0\u4f9b\u9060\u7aef\u547c\u53eb\uff1a"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# examples/llm_brick_define/grpc_server.py\nfrom my_brick import MyLLMBrick\nfrom llmbrick.servers.grpc.server import GrpcServer\n\ngrpc_server = GrpcServer(port=50051)\nmy_brick = MyLLMBrick(\n    default_prompt="gRPC default prompt",\n    model_id="grpc-llm",\n    supported_languages=["en", "zh"],\n    version="1.0.0",\n    description="gRPC LLMBrick example"\n)\ngrpc_server.register_service(my_brick)\n\nif __name__ == "__main__":\n    grpc_server.run()\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"5-\u4ee5-grpc-client-\u547c\u53eb\u9060\u7aef-llmbrick",children:"5. \u4ee5 gRPC Client \u547c\u53eb\u9060\u7aef LLMBrick"}),"\n",(0,i.jsxs)(r.p,{children:["\u53ef\u900f\u904e ",(0,i.jsx)(r.code,{children:"LLMBrick.toGrpcClient"})," \u7522\u751f\u9060\u7aef\u4ee3\u7406\u7269\u4ef6\uff0c\u4e26\u4ee5 async \u65b9\u5f0f\u547c\u53eb\u5404\u7a2e\u65b9\u6cd5\uff1a"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# examples/llm_brick_define/grpc_client.py\nfrom my_brick import MyLLMBrick\nfrom llmbrick.protocols.models.bricks.llm_types import LLMRequest, Context\n\nif __name__ == "__main__":\n    import asyncio\n\n    # \u5efa\u7acb gRPC client\n    my_brick = MyLLMBrick.toGrpcClient(\n        remote_address="127.0.0.1:50051",\n        default_prompt="gRPC client prompt",\n        model_id="grpc-client-llm"\n    )\n\n    print("=== Get Service Info ===")\n    def run_get_service_info_example():\n        async def example():\n            info = await my_brick.run_get_service_info()\n            print(info)\n        asyncio.run(example())\n\n    run_get_service_info_example()\n\n    print("\\n=== Unary Method ===")\n    def run_unary_example(is_test_error=False):\n        async def example():\n            if is_test_error:\n                req = LLMRequest(prompt="", context=[Context(role="user", content="")])\n            else:\n                req = LLMRequest(prompt="Hello from gRPC client", context=[Context(role="user", content="Hi")])\n            resp = await my_brick.run_unary(req)\n            print(resp)\n        asyncio.run(example())\n\n    print("Normal case:")\n    run_unary_example(is_test_error=False)\n    print("Error case:")\n    run_unary_example(is_test_error=True)\n\n    print("\\n=== Output Streaming Method ===")\n    def run_output_streaming_example(is_test_error=False):\n        async def example():\n            if is_test_error:\n                req = LLMRequest(prompt="", context=[])\n            else:\n                req = LLMRequest(prompt="Stream this via gRPC", context=[])\n            async for resp in my_brick.run_output_streaming(req):\n                await asyncio.sleep(0.2)\n                print(resp)\n        asyncio.run(example())\n\n    print("Normal case:")\n    run_output_streaming_example(is_test_error=False)\n    print("Error case:")\n    run_output_streaming_example(is_test_error=True)\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"6-\u65b9\u6cd5\u578b\u614b\u7e3d\u89bd",children:"6. \u65b9\u6cd5\u578b\u614b\u7e3d\u89bd"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"\u65b9\u6cd5\u578b\u614b"}),(0,i.jsx)(r.th,{children:"\u88dd\u98fe\u5668"}),(0,i.jsx)(r.th,{children:"\u8aaa\u660e"}),(0,i.jsx)(r.th,{children:"\u7bc4\u4f8b\u547c\u53eb\u65b9\u5f0f"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Unary"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"@unary_handler"})}),(0,i.jsx)(r.td,{children:"\u4e00\u6b21\u8acb\u6c42/\u56de\u61c9"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"await run_unary(request)"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Output Streaming"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"@output_streaming_handler"})}),(0,i.jsx)(r.td,{children:"\u4e00\u6b21\u8f38\u5165\uff0c\u591a\u6b21\u56de\u61c9"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"async for r in run_output_streaming(request)"})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Service Info"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"@get_service_info_handler"})}),(0,i.jsx)(r.td,{children:"\u67e5\u8a62\u670d\u52d9\u8cc7\u8a0a"}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.code,{children:"await run_get_service_info()"})})]})]})]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"7-\u5be6\u4f5c\u5efa\u8b70\u8207\u6700\u4f73\u5be6\u8e10",children:"7. \u5be6\u4f5c\u5efa\u8b70\u8207\u6700\u4f73\u5be6\u8e10"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"\u578b\u5225\u8a3b\u8a18"}),"\uff1a\u5efa\u8b70\u660e\u78ba\u6a19\u8a3b\u6240\u6709\u65b9\u6cd5\u7684\u8f38\u5165/\u8f38\u51fa\u578b\u5225\uff0c\u63d0\u5347\u53ef\u8b80\u6027\u8207\u7dad\u8b77\u6027\u3002"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"\u932f\u8aa4\u8655\u7406"}),"\uff1a\u5584\u7528 ",(0,i.jsx)(r.code,{children:"ErrorDetail"})," \u56de\u50b3\u6a19\u6e96\u5316\u932f\u8aa4\u8cc7\u8a0a\uff0c\u65b9\u4fbf\u524d\u5f8c\u7aef\u5354\u4f5c\u3002"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"\u975e\u540c\u6b65\u8a2d\u8a08"}),"\uff1a\u6240\u6709\u65b9\u6cd5\u7686\u5efa\u8b70\u4f7f\u7528 async/await\uff0c\u78ba\u4fdd\u9ad8\u6548\u80fd\u8207\u53ef\u64f4\u5145\u6027\u3002"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"\u6d41\u5f0f\u8655\u7406"}),"\uff1a\u6d41\u5f0f\u65b9\u6cd5\u53ef\u7528\u65bc\u5927\u91cf\u8cc7\u6599\u3001\u9577\u6642\u9593\u4efb\u52d9\u7b49\u5834\u666f\uff0c\u5584\u7528 async generator\u3002"]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"8-\u5b8c\u6574\u7bc4\u4f8b\u7a0b\u5f0f\u78bc",children:"8. \u5b8c\u6574\u7bc4\u4f8b\u7a0b\u5f0f\u78bc"}),"\n",(0,i.jsxs)(r.p,{children:["\u8acb\u53c3\u8003 ",(0,i.jsx)(r.a,{href:"https://github.com/JiHungLin/llmbrick/tree/main/examples/llm_brick_define",children:(0,i.jsx)(r.code,{children:"examples/llm_brick_define/"})})," \u76ee\u9304\u4e0b\u7684\u5b8c\u6574\u7bc4\u4f8b\uff0c\u5305\u542b\u672c\u5730\u7aef\u8207 gRPC \u5169\u7a2e\u7528\u6cd5\uff0c\u4e26\u6db5\u84cb\u6240\u6709\u5e38\u898b\u65b9\u6cd5\u578b\u614b\u3002"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"9-\u5e38\u898b\u554f\u984c",children:"9. \u5e38\u898b\u554f\u984c"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Q: \u5982\u4f55\u64f4\u5145\u81ea\u8a02\u6b04\u4f4d\uff1f"}),(0,i.jsx)(r.br,{}),"\n","A: \u65bc ",(0,i.jsx)(r.code,{children:"MyLLMBrick.__init__"})," \u6216\u5404\u65b9\u6cd5\u4e2d\u81ea\u8a02\u5c6c\u6027\u8207\u908f\u8f2f\u5373\u53ef\uff0c\u4e26\u53ef\u900f\u904e ",(0,i.jsx)(r.code,{children:"LLMRequest"})," \u7684 ",(0,i.jsx)(r.code,{children:"prompt"}),"\u3001",(0,i.jsx)(r.code,{children:"context"})," \u50b3\u905e\u81ea\u8a02\u8cc7\u6599\u3002"]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.p,{children:"\u672c\u6559\u5b78\u6db5\u84cb\u4e86 LLMBrick \u7684\u5b8c\u6574\u5b9a\u7fa9\u3001\u5be6\u4f5c\u8207\u4f7f\u7528\u6d41\u7a0b\uff0c\u9069\u5408\u521d\u5b78\u8005\u8207\u9032\u968e\u958b\u767c\u8005\u5feb\u901f\u4e0a\u624b LLMBrick \u6846\u67b6\u7684\u81ea\u8a02 LLM \u6a21\u7d44\u958b\u767c\u3002"})]})}function a(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>l});var t=n(6540);const i={},s=t.createContext(i);function o(e){const r=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:r},e.children)}}}]);