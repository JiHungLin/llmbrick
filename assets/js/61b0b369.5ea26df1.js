"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[452],{4922:(e,n,r)=>{r.r(n),r.d(n,{default:()=>a});var o=r(6540),s=r(1656),t=r(4848);const i='LLMBrick: Comprehensive Guide to Custom Brick Development and Library Usage\n\nOverview and Purpose\nLLMBrick is a modular Python framework designed to solve the challenges of building, composing, and deploying advanced LLM (Large Language Model) applications. It addresses the need for reusable, composable, and maintainable AI components by introducing the concept of \u201cBricks\u201d\u2014well-defined, pluggable modules that encapsulate specific logic, workflows, or integrations. LLMBrick enables rapid prototyping, scalable deployment, and robust error handling for LLM-powered systems.\n\nCore Features\n- Modular Brick system: Each Brick is a self-contained component for tasks like intent detection, text correction, retrieval, translation, or integrating external APIs.\n- Standardized protocols: Unified request/response types, error handling, and streaming support (unary, input/output/bidirectional streaming).\n- Service deployment: Bricks can be exposed as gRPC or SSE services for distributed or real-time applications.\n- Extensibility: Easily create custom Bricks or extend built-in ones.\n- Async/await support for high concurrency and non-blocking operations.\n- Built-in error codes, logging, and performance monitoring utilities.\n\nBrick System: Base Bricks and Their Interfaces\n\nLLMBrick provides several base Bricks, each designed for a specific NLP or orchestration task. All Bricks inherit from a common interface and expose a set of standard handler functions. Below is an overview of the main base Bricks, their request/response types, and customizable functions:\n\n1. **CommonBrick**\n   - Purpose: General-purpose base for custom Bricks.\n   - Handlers: \n     - `run_unary(request: CommonRequest) -> CommonResponse`\n     - `run_input_streaming(request_stream: AsyncIterable[CommonRequest]) -> CommonResponse`\n     - `run_output_streaming(request: CommonRequest) -> AsyncIterable[CommonResponse]`\n     - `run_bidi_streaming(request_stream: AsyncIterable[CommonRequest]) -> AsyncIterable[CommonResponse]`\n     - `run_get_service_info() -> ServiceInfo`\n   - Request: `CommonRequest` (data: dict, metadata: dict)\n   - Response: `CommonResponse` (data: dict, error: ErrorDetail)\n   - Customizable: Override any handler, add custom logic, define new methods.\n\n2. **LLMBrick**\n   - Purpose: Integrate and manage LLM (e.g., OpenAI) calls.\n   - Handlers:\n     - `run_unary(request: LLMRequest) -> LLMResponse`\n     - `run_output_streaming(request: LLMRequest) -> AsyncIterable[LLMResponse]`\n   - Request: `LLMRequest` (prompt, context, parameters)\n   - Response: `LLMResponse` (text, tokens, error)\n   - Customizable: Model selection, prompt engineering, output formatting.\n\n3. **ComposeBrick**\n   - Purpose: Orchestrate multiple Bricks in a pipeline or workflow.\n   - Handlers:\n     - `run_unary(request: ComposeRequest) -> ComposeResponse`\n   - Request: `ComposeRequest` (steps, data)\n   - Response: `ComposeResponse` (results, error)\n   - Customizable: Define step logic, data flow, error handling between Bricks.\n\n4. **GuardBrick**\n   - Purpose: Input validation, filtering, and safety checks.\n   - Handlers:\n     - `run_unary(request: GuardRequest) -> GuardResponse`\n   - Request: `GuardRequest` (input, rules)\n   - Response: `GuardResponse` (is_valid, reason, error)\n   - Customizable: Validation rules, filtering logic.\n\n5. **IntentionBrick**\n   - Purpose: Intent detection and classification.\n   - Handlers:\n     - `run_unary(request: IntentionRequest) -> IntentionResponse`\n   - Request: `IntentionRequest` (text, context)\n   - Response: `IntentionResponse` (intent, confidence, error)\n   - Customizable: Intent schema, classification logic.\n\n6. **RectifyBrick**\n   - Purpose: Text correction and normalization.\n   - Handlers:\n     - `run_unary(request: RectifyRequest) -> RectifyResponse`\n   - Request: `RectifyRequest` (text, language)\n   - Response: `RectifyResponse` (corrected_text, error)\n   - Customizable: Correction algorithms, language support.\n\n7. **RetrievalBrick**\n   - Purpose: Information retrieval from knowledge bases or documents.\n   - Handlers:\n     - `run_unary(request: RetrievalRequest) -> RetrievalResponse`\n   - Request: `RetrievalRequest` (query, top_k, filters)\n   - Response: `RetrievalResponse` (documents, scores, error)\n   - Customizable: Retrieval backend, ranking logic.\n\n8. **TranslateBrick**\n   - Purpose: Multilingual translation.\n   - Handlers:\n     - `run_unary(request: TranslateRequest) -> TranslateResponse`\n   - Request: `TranslateRequest` (text, source_lang, target_lang)\n   - Response: `TranslateResponse` (translated_text, error)\n   - Customizable: Supported languages, translation engine.\n\nHandler Decorators and Customization\n- `@unary_handler`: For single request/response logic.\n- `@input_streaming_handler`: For processing a stream of input requests.\n- `@output_streaming_handler`: For streaming multiple outputs from a single request.\n- `@bidi_streaming_handler`: For bidirectional streaming scenarios.\n- `@get_service_info_handler`: For exposing Brick metadata and capabilities.\n\nAll handlers are async and can be overridden to implement custom logic. You can add additional methods or properties as needed.\n\nRequest/Response Data Types\n- All Bricks use standardized request/response types, typically with a `.data` dictionary for inputs/outputs and an `error` field for status.\n- Refer to the protocols in `llmbrick.protocols.models.bricks` for detailed type definitions.\n\nExtending and Customizing Bricks\n- Inherit from any base Brick to add or override handler methods.\n- Document the expected input/output schema for your Brick.\n- Compose Bricks for complex workflows using ComposeBrick or by chaining outputs/inputs.\n\nService Deployment and Integration\n- Register any Brick as a gRPC or SSE service for distributed or real-time applications.\n- Configure service parameters (host, port, etc.) as needed.\n\nQuick Example: Minimal Brick\n```python\nfrom llmbrick.bricks.common.common import CommonBrick\nfrom llmbrick.core.brick import unary_handler\nfrom llmbrick.protocols.models.bricks.common_types import CommonRequest, CommonResponse, ErrorDetail\nfrom llmbrick.core.error_codes import ErrorCodes\n\nclass HelloBrick(CommonBrick):\n    @unary_handler\n    async def hello(self, request: CommonRequest) -> CommonResponse:\n        name = request.data.get("name", "World")\n        return CommonResponse(\n            data={"message": f"Hello, {name}!"},\n            error=ErrorDetail(code=ErrorCodes.SUCCESS, message="Success")\n        )\n```\nTest locally:\n```python\nimport asyncio\nfrom hello_brick import HelloBrick\nfrom llmbrick.protocols.models.bricks.common_types import CommonRequest\n\nasync def main():\n    brick = HelloBrick()\n    req = CommonRequest(data={"name": "Alice"})\n    resp = await brick.hello(req)\n    print(resp.data["message"])  # Output: Hello, Alice!\n\nif __name__ == "__main__":\n    asyncio.run(main())\n```\n\nService Deployment Example (gRPC)\n```python\nfrom llmbrick.servers.grpc.server import GrpcServer\nfrom hello_brick import HelloBrick\n\nbrick = HelloBrick()\nserver = GrpcServer(port=50051)\nserver.register_service(brick)\nserver.run()\n```\n\nService Deployment Example (SSE)\n```python\nfrom llmbrick.servers.sse import SSEServer\nfrom hello_brick import HelloBrick\n\nbrick = HelloBrick()\nserver = SSEServer(brick, host="0.0.0.0", port=8000, enable_test_page=True)\nserver.run()\n```\n\nAdvanced Brick Composition Example\n```python\nfrom llmbrick.bricks.guard.base_guard import GuardBrick\nfrom llmbrick.bricks.rectify.base_rectify import RectifyBrick\nfrom llmbrick.bricks.intention.base_intention import IntentionBrick\nfrom llmbrick.bricks.llm.base_llm import LLMBrick\n\nguard_brick = GuardBrick()\nrectify_brick = RectifyBrick()\nintention_brick = IntentionBrick()\nllm_brick = LLMBrick()\n\nasync def main():\n    result1 = await guard_brick.run_unary(guard_request)\n    rectify_text = await rectify_brick.run_unary(rectify_request)\n    intention_result = await intention_brick.run_unary(rectify_text)\n    async for answer in llm_brick.run_output_streaming(question_context):\n        print(answer)\n```\n\nException Handling and Using ErrorCodes\n\nLLMBrick provides a robust error handling mechanism using standardized error codes and the `ErrorDetail` structure. Each response includes an `error` field, which contains a code and message. The `llmbrick.core.error_codes.ErrorCodes` module defines common error codes such as `SUCCESS`, `INVALID_INPUT`, `NOT_FOUND`, `INTERNAL_ERROR`, etc.\n\nHow to Use ErrorCodes in Your Brick:\n- Always set the appropriate error code and message in your response.\n- Use try/except blocks in your handler methods to catch exceptions and return meaningful error information.\n- For custom errors, define your own codes or use the provided ones for consistency.\n\nExample: Error Handling in a Custom Brick\n```python\nfrom llmbrick.core.error_codes import ErrorCodes\n\nclass MyCustomBrick(CommonBrick):\n    @unary_handler\n    async def process(self, request: CommonRequest) -> CommonResponse:\n        try:\n            user_input = request.data.get("input", "")\n            if not user_input:\n                return CommonResponse(\n                    data={},\n                    error=ErrorDetail(code=ErrorCodes.INVALID_INPUT, message="Input is required")\n                )\n            # Your logic here\n            result = user_input.upper()\n            return CommonResponse(\n                data={"result": result},\n                error=ErrorDetail(code=ErrorCodes.SUCCESS, message="Success")\n            )\n        except Exception as e:\n            return CommonResponse(\n                data={},\n                error=ErrorDetail(code=ErrorCodes.INTERNAL_ERROR, message=str(e))\n            )\n```\n- Always check the `error` field in the response when consuming a Brick\u2019s output.\n- Use error codes to implement retry logic, user feedback, or escalation as needed.\n';function a(){const e=(0,o.useRef)(null),[n,r]=(0,o.useState)(!1);return(0,t.jsx)(s.A,{title:"LLMBrick Prompt",children:(0,t.jsxs)("div",{style:{maxWidth:900,margin:"0 auto",padding:"2rem 1rem"},children:[(0,t.jsx)("h1",{children:"LLMBrick Prompt"}),(0,t.jsxs)("div",{style:{display:"flex",alignItems:"center",gap:"1rem",marginBottom:"1rem"},children:[(0,t.jsx)("button",{onClick:()=>{e.current&&(navigator.clipboard.writeText(i),r(!0),setTimeout(()=>r(!1),1500))},style:{padding:"0.5rem 1.2rem",fontSize:"1rem",cursor:"pointer",background:"#3578e5",color:"white",border:"none",borderRadius:4},children:"\u4e00\u9375\u8907\u88fd"}),n&&(0,t.jsx)("span",{style:{color:"#3578e5",fontWeight:600,fontSize:"1rem"},children:"\u5df2\u8907\u88fd!"})]}),(0,t.jsx)("pre",{ref:e,style:{background:"#222",color:"#fff",padding:"1.5rem",borderRadius:8,overflowX:"auto",fontSize:"0.95rem",lineHeight:1.5,maxHeight:600},children:i})]})})}}}]);